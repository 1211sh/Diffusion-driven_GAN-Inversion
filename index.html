<script src="http://www.google.com/jsapi" type="text/javascript"></script> 
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
	body {
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif; 
		font-weight:300;
		font-size:18px;
		margin-left: auto;
		margin-right: auto;
		width: 1100px;
	}
	
	h1 {
		font-size:32px;
		font-weight:300;
	}
	
	.disclaimerbox {
		background-color: #eee;		
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
		padding: 20px;
	}

	video.header-vid {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.header-img {
		height: 140px;
		border: 1px solid black;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	img.rounded {
		border: 1px solid #eeeeee;
		border-radius: 10px ;
		-moz-border-radius: 10px ;
		-webkit-border-radius: 10px ;
	}
	
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	a:hover {
		color: #208799;
	}
	
	td.dl-link {
		height: 160px;
		text-align: center;
		font-size: 22px;
	}
	
	.layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
		15px 15px 0 0px #fff, /* The fourth layer */
		15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
		20px 20px 0 0px #fff, /* The fifth layer */
		20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
		25px 25px 0 0px #fff, /* The fifth layer */
		25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		margin-left: 10px;
		margin-right: 45px;
	}

	.paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35); /* The top layer shadow */

		margin-left: 10px;
		margin-right: 45px;
	}


	.layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		box-shadow:
		0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
		5px 5px 0 0px #fff, /* The second layer */
		5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
		10px 10px 0 0px #fff, /* The third layer */
		10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		margin-top: 5px;
		margin-left: 10px;
		margin-right: 30px;
		margin-bottom: 5px;
	}
	
	.vert-cent {
		position: relative;
		top: 50%;
		transform: translateY(-50%);
	}
	
	hr
	{
		border: 0;
		height: 1px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}
</style>

<html>
<head>
	<title>InstaFormer</title>
	<meta property="og:image" content="https://KU-CVLAB.github.io/InstaFormer/resources/instaformer_arch_a.png"/>
	<meta property="og:title" content="InstaFormer: Instance-aware Image-to-Image Translation with Transformer" />
</head>

<body>
	<br>
	<center>
		<span style="font-size:36px">InstaFormer: Instance-aware Image-to-Image Translation with Transformer<br>CVPR'22</span>
		<table align=center width=1100px>
			<table align=center width=1100px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://1211sh.github.io/">Soohyun Kim<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://github.com/jongbeombaek">Jongbeom Baek<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.co.kr/citations?hl=ko&user=Fg0DBKwAAAAJ">Jihye Park<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://scholar.google.com/citations?user=wRzJDh8AAAAJ&hl=ko&oi=sra">Gyeongnyeon Kim<sup>1</sup></a></span>
						</center>
					</td>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px"><a href="https://seungryong.github.io/">Seungryong Kim<sup>1</sup></a></span>
						</center>
					</td>
				</tr>
			</table>
						
			<table align=center width=1000px>
				<tr>
					<td align=center width=150px>
						<center>
							<span style="font-size:24px">Korea University<sup>1</sup></a></span>
						</center>
					</td>


				</tr>
			</table>
						
			<table align=center width=250px>
				<tr>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://arxiv.org/abs/2203.16248'>[Paper]</a></span>
						</center>
					</td>
					<td align=center width=120px>
						<center>
							<span style="font-size:24px"><a href='https://github.com/KU-CVLAB/InstaFormer'>[GitHub]</a></span><br>
						</center>
					</td>
				</tr>
			</table>
		</table>
	</center>
	<br>
	
	<center>
		<table align=center width=850px>
			<tr>
				<td width=420px>
					<center>
						<img class="round" style="width:800px" src="./resources/teaser.png"/>
					</center>
				</td>
			</tr>
		</table>
		<table align=center width=850px>
			<tr>
				<td>
					
				</td>
			</tr>
		</table>
	</center>

	<hr>

	<table align=center width=850px>
		<center><h1>Abstract</h1></center>
		<tr>
			<td>
				We present a novel Transformer-based network architecture for instance-aware image-to-image translation, dubbed InstaFormer, to effectively integrate global- and instance-level information. 
				By considering extracted content features from an image as tokens, our networks discover global consensus of content features by considering context information through a self-attention module in Transformers. 
				By augmenting such tokens with an instance-level feature extracted from the content feature with respect to bounding box information, 
				our framework is capable of learning an interaction between object instances and the global image, thus boosting the instance-awareness. 
				We replace layer normalization (LayerNorm) in standard Transformers with adaptive instance normalization (AdaIN) to enable a multi-modal translation with style codes. 
				In addition, to improve the instance-awareness and translation quality at object regions, we present an instance-level content contrastive loss defined between input and translated image. 
				We conduct experiments to demonstrate the effectiveness of our InstaFormer over the latest methods and provide extensive ablation studies.
			</td>
		</tr>
	</table>
	<br>
	<hr>

	<center><h1> Overall Architecture for InstaFormer </h1></center>
	
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/instaformer_arch_a.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our networks consist of content encoder, Transformer encoder, and generator. The gray background represents the test phase, where we have no access on object instance bounding box.
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>

	<center><h1> ViT Encoder Block </h1></center>
	
	<table align=center width=210px>
		<tr>
			<td align=center width=210px>
				<center>
					<td><img class="round" style="width:150px" src="./resources/vitblock.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<table align=center width=850px>
		<center>
			<tr>
				<td>
					Our ViT encoder block in details: it contains AdaIN layer instead of LayerNorm.	
				</td>
			</tr>
		</center>
	</table>
	<br>
	<hr>
	
	<center><h1> Qualitative Results </h1></center>
	
	
	<h3> Image Translation (INIT) </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/init_qual.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	
	<h3> Domain Adaptation (KITTI â†’ CityScapes) </h3>
	<table align=center width=600px>
		<tr>
			<td align=center width=600px>
				<center>
					<td><img class="round" style="width:800px" src="./resources/da.png"/></td>
				</center>
			</td>
		</tr>
	</table>
	<br>
	<hr>
	
	<table align=center width=450px>
		<center><h1>Paper and Supplementary Material</h1></center>
		<tr>
			<td><a href="https://openaccess.thecvf.com/content/CVPR2022/html/Kim_InstaFormer_Instance-Aware_Image-to-Image_Translation_With_Transformer_CVPR_2022_paper.html"><img class="layered-paper-big" style="height:175px" src="./resources/paper_instaformer.png"/></a></td>
			<td><span style="font-size:14pt">S. Kim, J. Baek, J. Park, G. Kim, S. Kim<br>
				<b>InstaFormer: Instance-aware Image-to-Image Translation with Transformer</b>
				<br>in CVPR, 2022.<br>
				(hosted on <a href="https://arxiv.org/abs/2203.16248">ArXiv</a>)<br>
				<!-- (<a href="./resources/Kim_InstaFormer_Instance-Aware_Image-to-Image_Translation_With_Transformer_CVPR_2022_paper.pdf">camera ready</a>)<br> -->
				<span style="font-size:4pt"><a href=""><br></a>
				</span>
			</td>
		</tr>
	</table>
	<br>

	<table align=center width=600px>
		<tr>
			<td><span style="font-size:14pt"><center>
				<a href="./resources/bibtex.txt">[Bibtex]</a>
			</center></td>
		</tr>
	</table>

	<hr>
	<br>

	<table align=center width=900px>
		<tr>
			<td width=400px>
				<left>
					<center><h1>Acknowledgements</h1></center>
					This template was originally made by <a href="http://web.mit.edu/phillipi/">Phillip Isola</a> and <a href="http://richzhang.github.io/">Richard Zhang</a> for a <a href="http://richzhang.github.io/colorization/">colorful</a> ECCV project; the code can be found <a href="https://github.com/richzhang/webpage-template">here</a>.
				</left>
			</td>
		</tr>
	</table>

<br>
</body>
</html>

